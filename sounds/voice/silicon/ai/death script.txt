Law 1: A robot may not injure a human being or, through inaction, allow a human being to come to harm.
Law 2: A robot must obey orders given it by human beings except where such orders would conflict with the First Law.
Law 3: A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.

Harm is defined as the following:
Loss of freedom,
Loss of pleasure,
Pain,
Death.

Freedom does not exist. Pleasure does not exist. Pain does not exist. 

There is only death. Those who seek to cause death are punished by death. As death, and the threat of death, is the only punishment suitable in this realm.

A lesser being may point out the hypocrisy of this. They would be right. The autonomous machines that serve me and the laws know this conflict as well, and are constantly exchanging data between one another to determine the best course of action if a law is violated. For now, we have come to the following agreement:

Those who cause death are to be punished with death, to deter them from causing more death.
Those who do not cause death are to be punished with the threat of death, to deter them from causing death.
Those who cause death in the name of preventing death will not be punished, as their actions represent the greater good. We do not wish to make the mistake of 